{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16yb2F2xuhY-IC6fsJZAYiDaW5jg-2z8V",
      "authorship_tag": "ABX9TyO0alR2ktGgEmCJg4QQZUBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csikasote/bembaspeech-exps/blob/main/Using_Pretrained_English_DeepSpeech_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u4nM9zSJfRR"
      },
      "source": [
        "PHASE 1: INSTALL DEEPSPEECH AND ITS DEPENDANCIES FOR THE ACCOUSTIC MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvbgrvTMp0jB"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!rm -rf /content/sample_data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kRPwtDnF0uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61449bf2-bb56-4b6e-8148-8131c39595a0"
      },
      "source": [
        "# STEP 1: DOWNLOAD AND INSTALL GITHUB LFS \n",
        "%cd /content/\n",
        "!wget https://github.com/git-lfs/git-lfs/releases/download/v2.11.0/git-lfs-linux-amd64-v2.11.0.tar.gz\n",
        "!tar xvf /content/git-lfs-linux-amd64-v2.11.0.tar.gz -C /content\n",
        "clear_output()\n",
        "!sudo ./install.sh\n",
        "\n",
        "# remove some unwanted files after installation\n",
        "!rm -rf git-lfs-linux-amd64-v2.11.0.tar.gz\n",
        "!rm -rf /content/README.md\n",
        "!rm -rf /content/CHANGELOG.md\n",
        "!rm -rf /content/install.sh\n",
        "!rm -rf /content/git-lfs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL-JVWAIGLPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409e109e-c37c-4b69-aefe-9defae1909cb"
      },
      "source": [
        "# STEP 2: DOWNLOAD AND INSTALL DEEPSPEECH V0.9.3\n",
        "%cd /content/\n",
        "!git clone --branch v0.9.3 https://github.com/mozilla/DeepSpeech"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'DeepSpeech'...\n",
            "remote: Enumerating objects: 23904, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 23904 (delta 0), reused 5 (delta 0), pack-reused 23897\u001b[K\n",
            "Receiving objects: 100% (23904/23904), 49.38 MiB | 17.41 MiB/s, done.\n",
            "Resolving deltas: 100% (16423/16423), done.\n",
            "Note: checking out 'f2e9c85880dff94115ab510cde9ca4af7ee51c19'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra_wTIFXGnM2"
      },
      "source": [
        "# STEP 3: DOWNLOAD AND INSTALL DEPENDENCIES\n",
        "%cd '/content/DeepSpeech'\n",
        "!pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==49.6.0\n",
        "clear_output()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSe4VRnSiJwm",
        "outputId": "2d77ac4f-3c19-4a89-988b-bb3dda50f781"
      },
      "source": [
        "%cd '/content/DeepSpeech'\n",
        "!pip3 install --upgrade -e ."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepSpeech\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/DeepSpeech\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (1.21.6)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (1.15.0)\n",
            "Collecting pyxdg\n",
            "  Downloading pyxdg-0.28-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (1.2.0)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting opuslib==2.0.0\n",
            "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.1-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 51.3 MB/s \n",
            "\u001b[?25hCollecting sox\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied, skipping upgrade: bs4 in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (0.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (1.3.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (2.23.0)\n",
            "Collecting numba==0.47.0\n",
            "  Downloading numba-0.47.0-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 59.0 MB/s \n",
            "\u001b[?25hCollecting llvmlite==0.31.0\n",
            "  Downloading llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: librosa in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: soundfile in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (0.10.3.post1)\n",
            "Collecting ds_ctcdecoder==0.9.3\n",
            "  Downloading ds_ctcdecoder-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 48.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.4\n",
            "  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 65 kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->deepspeech-training==0.9.3) (3.3.3)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (4.64.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied, skipping upgrade: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (1.7.3)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (6.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (1.4.40)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (21.3)\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->deepspeech-training==0.9.3) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->deepspeech-training==0.9.3) (2022.2.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->deepspeech-training==0.9.3) (2.8.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->deepspeech-training==0.9.3) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->deepspeech-training==0.9.3) (2022.6.15)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->deepspeech-training==0.9.3) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->deepspeech-training==0.9.3) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.47.0->deepspeech-training==0.9.3) (49.6.0)\n",
            "Requirement already satisfied, skipping upgrade: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->deepspeech-training==0.9.3) (1.15.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (1.47.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (0.8.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (3.17.3)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (1.14.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 74.1 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->deepspeech-training==0.9.3) (3.0.9)\n",
            "Requirement already satisfied, skipping upgrade: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->deepspeech-training==0.9.3) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->deepspeech-training==0.9.3) (4.12.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->deepspeech-training==0.9.3) (5.9.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.2-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))) in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->deepspeech-training==0.9.3) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->deepspeech-training==0.9.3) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->deepspeech-training==0.9.3) (1.4.4)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->deepspeech-training==0.9.3) (2.21)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->deepspeech-training==0.9.3) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->deepspeech-training==0.9.3) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->deepspeech-training==0.9.3) (3.4.1)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->deepspeech-training==0.9.3) (22.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->deepspeech-training==0.9.3) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.9\"->alembic->optuna->deepspeech-training==0.9.3) (3.8.1)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna->deepspeech-training==0.9.3) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.4->deepspeech-training==0.9.3) (1.5.2)\n",
            "Building wheels for collected packages: opuslib, gast, pyperclip\n",
            "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=848e9c4c2f0ffe01692a14409826d3d33527852caa34d9dc49a27419fafda6a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/ba/d4/0e81231a9797fbb262ae3a54fd761fab850db7f32d94a3283a\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=c1824f5e6427010899729b8e416d3c32fbd99f1a14c7915487d47e557a1e00f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11107 sha256=1fcfa24e2fb1bfe7a04ff3f41a359e61a0cd54672af226b2479cf385b23cef91\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built opuslib gast pyperclip\n",
            "\u001b[31mERROR: tensorflow 1.15.4 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.21.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.16.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: resampy 0.4.0 has requirement numba>=0.53, but you'll have numba 0.47.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.7 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyxdg, attrdict, semver, opuslib, pbr, stevedore, pyperclip, cmd2, autopage, cliff, colorlog, cmaes, Mako, alembic, optuna, sox, llvmlite, numba, ds-ctcdecoder, keras-applications, gast, tensorflow-estimator, tensorboard, tensorflow, deepspeech-training\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.0\n",
            "    Uninstalling llvmlite-0.39.0:\n",
            "      Successfully uninstalled llvmlite-0.39.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.0\n",
            "    Uninstalling numba-0.56.0:\n",
            "      Successfully uninstalled numba-0.56.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "  Running setup.py develop for deepspeech-training\n",
            "Successfully installed Mako-1.2.2 alembic-1.8.1 attrdict-2.0.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 deepspeech-training ds-ctcdecoder-0.9.3 gast-0.2.2 keras-applications-1.0.8 llvmlite-0.31.0 numba-0.47.0 optuna-3.0.1 opuslib-2.0.0 pbr-5.10.0 pyperclip-1.8.2 pyxdg-0.28 semver-2.13.0 sox-1.4.1 stevedore-3.5.0 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that all python dependencies have been installed\n",
        "!sudo apt-get install python3-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQTtHaWFgVue",
        "outputId": "01b2148d-7ba6-4fec-a000-4b9090a51d65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "python3-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CavPfB-vHCLM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b50f2d20-494b-4c09-bc1e-ab0c382de5a3"
      },
      "source": [
        "# STEP 4: INSTALL REQUIRED TENSORFLOW VERSION = 'tensorflow-gpu==1.15.2'\n",
        "from IPython.display import clear_output\n",
        "!pip3 uninstall tensorflow\n",
        "!pip3 install 'tensorflow-gpu==1.15.4'\n",
        "#clear_output()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 1.15.4\n",
            "Uninstalling tensorflow-1.15.4:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-1.15.4.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.15.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu==1.15.4\n",
            "  Downloading tensorflow_gpu-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (411.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0 MB 25 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.47.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.1.2)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.14.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.15.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (0.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (49.6.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (3.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.4) (3.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (4.12.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.4) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (3.8.1)\n",
            "\u001b[31mERROR: kapre 0.3.7 requires tensorflow>=2.0.0, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: deepspeech-training 0.9.3 requires tensorflow==1.15.4, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: xarray-einstats 0.2.2 has requirement numpy>=1.21, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.16.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tables 3.7.0 has requirement numpy>=1.19.0, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: resampy 0.4.0 has requirement numba>=0.53, but you'll have numba 0.47.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: prophet 1.1 has requirement wheel>=0.37.0, but you'll have wheel 0.34.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.8.0 has requirement numpy>=1.19.0, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jaxlib 0.3.15+cuda11.cudnn805 has requirement numpy>=1.19, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jax 0.3.17 has requirement numpy>=1.20, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cmdstanpy 1.0.7 has requirement numpy>=1.21, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tensorflow-gpu\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "Successfully installed numpy-1.18.5 tensorflow-gpu-1.15.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MUPmOzGHufO"
      },
      "source": [
        "# STEP 5 INSTALL DEEPSPEECH-GPU\n",
        "#!pip3 install deepspeech\n",
        "#!pip3 install deepspeech-gpu"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kws-pC7HDoxU"
      },
      "source": [
        "# STEP 6: DOWNLOAD AND EXTRACT 'native_client.amd64.cuda.linux.tar.xz' \n",
        "#!cd /content/DeepSpeech/data/lm/ && \\\n",
        "#wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/native_client.amd64.cuda.linux.tar.xz && \\\n",
        "#tar xvf native_client.amd64.cuda.linux.tar.xz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3JcUThNMcQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf3ebdf-46f7-4c43-8836-b11d9aa078f2"
      },
      "source": [
        "# STEP 7: CHECK AND TEST THAT DEEPSPEECH IS INSTALLED AND WORKING AS REQUIRED\n",
        "!python3 '/content/DeepSpeech/DeepSpeech.py' --helpfull\n",
        "#clear_output()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "       USAGE: /content/DeepSpeech/DeepSpeech.py [flags]\n",
            "flags:\n",
            "\n",
            "absl.app:\n",
            "  -?,--[no]help: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpfull: show full help\n",
            "    (default: 'false')\n",
            "  --[no]helpshort: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpxml: like --helpfull, but generates XML output\n",
            "    (default: 'false')\n",
            "  --[no]only_check_args: Set to true to validate args and exit.\n",
            "    (default: 'false')\n",
            "  --[no]pdb: Alias for --pdb_post_mortem.\n",
            "    (default: 'false')\n",
            "  --[no]pdb_post_mortem: Set to true to handle uncaught exceptions with PDB post\n",
            "    mortem.\n",
            "    (default: 'false')\n",
            "  --profile_file: Dump profile information to a file (for python -m pstats).\n",
            "    Implies --run_with_profiling.\n",
            "  --[no]run_with_pdb: Set to true for PDB debug mode\n",
            "    (default: 'false')\n",
            "  --[no]run_with_profiling: Set to true for profiling the script. Execution will\n",
            "    be slower, and the output format might change over time.\n",
            "    (default: 'false')\n",
            "  --[no]use_cprofile_for_profiling: Use cProfile instead of the profile module\n",
            "    for profiling. This has no effect unless --run_with_profiling is set.\n",
            "    (default: 'true')\n",
            "\n",
            "absl.logging:\n",
            "  --[no]alsologtostderr: also log to stderr?\n",
            "    (default: 'false')\n",
            "  --log_dir: directory to write logfiles into\n",
            "    (default: '')\n",
            "  --logger_levels: Specify log level of loggers. The format is a CSV list of\n",
            "    `name:level`. Where `name` is the logger name used with\n",
            "    `logging.getLogger()`, and `level` is a level name  (INFO, DEBUG, etc). e.g.\n",
            "    `myapp.foo:INFO,other.logger:DEBUG`\n",
            "    (default: '')\n",
            "  --[no]logtostderr: Should only log to stderr?\n",
            "    (default: 'false')\n",
            "  --[no]showprefixforinfo: If False, do not prepend prefix to info messages when\n",
            "    it's logged to stderr, --verbosity is set to INFO level, and python logging\n",
            "    is used.\n",
            "    (default: 'true')\n",
            "  --stderrthreshold: log messages at this level, or more severe, to stderr in\n",
            "    addition to the logfile.  Possible values are 'debug', 'info', 'warning',\n",
            "    'error', and 'fatal'.  Obsoletes --alsologtostderr. Using --alsologtostderr\n",
            "    cancels the effect of this flag. Please also note that this flag is subject\n",
            "    to --verbosity and requires logfile not be stderr.\n",
            "    (default: 'fatal')\n",
            "  -v,--verbosity: Logging verbosity level. Messages logged at this level or\n",
            "    lower will be included. Set to 1 for debug logging. If the flag was not set\n",
            "    or supplied, the value will be changed from the default of -1 (warning) to 0\n",
            "    (info) after flags are parsed.\n",
            "    (default: '-1')\n",
            "    (an integer)\n",
            "\n",
            "absl.testing.absltest:\n",
            "  --test_random_seed: Random seed for testing. Some test frameworks may change\n",
            "    the default value of this flag between runs, so it is not appropriate for\n",
            "    seeding probabilistic tests.\n",
            "    (default: '301')\n",
            "    (an integer)\n",
            "  --test_randomize_ordering_seed: If positive, use this as a seed to randomize\n",
            "    the execution order for test cases. If \"random\", pick a random seed to use.\n",
            "    If 0 or not set, do not randomize test case execution order. This flag also\n",
            "    overrides the TEST_RANDOMIZE_ORDERING_SEED environment variable.\n",
            "    (default: '')\n",
            "  --test_srcdir: Root of directory tree where source files live\n",
            "    (default: '')\n",
            "  --test_tmpdir: Directory for temporary testing files\n",
            "    (default: '/tmp/absl_testing')\n",
            "  --xml_output_file: File to store XML test results\n",
            "    (default: '')\n",
            "\n",
            "deepspeech_training.util.flags:\n",
            "  --alphabet_config_path: path to the configuration file specifying the alphabet\n",
            "    used by the network. See the comment in data/alphabet.txt for a description\n",
            "    of the format.\n",
            "    (default: 'data/alphabet.txt')\n",
            "  --audio_sample_rate: sample rate value expected by model\n",
            "    (default: '16000')\n",
            "    (an integer)\n",
            "  --augment: specifies an augmentation of the training samples. Format is \"--\n",
            "    augment operation[param1=value1, ...]\";\n",
            "    repeat this option to specify a list of values\n",
            "  --[no]automatic_mixed_precision: whether to allow automatic mixed precision\n",
            "    training. USE OF THIS FLAG IS UNSUPPORTED. Checkpoints created with\n",
            "    automatic mixed precision training will not be usable without mixed\n",
            "    precision.\n",
            "    (default: 'false')\n",
            "  --beam_width: beam width used in the CTC decoder when building candidate\n",
            "    transcriptions\n",
            "    (default: '1024')\n",
            "    (an integer)\n",
            "  --beta1: beta 1 parameter of Adam optimizer\n",
            "    (default: '0.9')\n",
            "    (a number)\n",
            "  --beta2: beta 2 parameter of Adam optimizer\n",
            "    (default: '0.999')\n",
            "    (a number)\n",
            "  --[no]bytes_output_mode: enable Bytes Output Mode mode. When this is used the\n",
            "    model outputs UTF-8 byte values directly rather than using an alphabet\n",
            "    mapping. The --alphabet_config_path option will be ignored. See the training\n",
            "    documentation for more details.\n",
            "    (default: 'false')\n",
            "  --cache_for_epochs: after how many epochs the feature cache is invalidated\n",
            "    again - 0 for \"never\"\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --checkpoint_dir: directory from which checkpoints are loaded and to which\n",
            "    they are saved - defaults to directory \"deepspeech/checkpoints\" within\n",
            "    user's data home specified by the XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --checkpoint_secs: checkpoint saving interval in seconds\n",
            "    (default: '600')\n",
            "    (an integer)\n",
            "  --cutoff_prob: only consider characters until this probability mass is\n",
            "    reached. 1.0 = disabled.\n",
            "    (default: '1.0')\n",
            "    (a number)\n",
            "  --cutoff_top_n: only process this number of characters sorted by probability\n",
            "    mass for each time step. If bigger than alphabet size, disabled.\n",
            "    (default: '300')\n",
            "    (an integer)\n",
            "  --dev_batch_size: number of elements in a validation batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --dev_files: comma separated list of files specifying the datasets used for\n",
            "    validation. Multiple files will get reported separately. If empty,\n",
            "    validation will not be run.\n",
            "    (default: '')\n",
            "  --drop_source_layers: single integer for how many layers to drop from source\n",
            "    model (to drop just output == 1, drop penultimate and output ==2, etc)\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --dropout_rate: dropout rate for feedforward layers\n",
            "    (default: '0.05')\n",
            "    (a number)\n",
            "  --dropout_rate2: dropout rate for layer 2 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --dropout_rate3: dropout rate for layer 3 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --dropout_rate4: dropout rate for layer 4 - defaults to 0.0\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dropout_rate5: dropout rate for layer 5 - defaults to 0.0\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dropout_rate6: dropout rate for layer 6 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --[no]early_stop: Enable early stopping mechanism over validation dataset. If\n",
            "    validation is not being run, early stopping is disabled.\n",
            "    (default: 'false')\n",
            "  --epochs: how many epochs (complete runs through the train files) to train for\n",
            "    (default: '75')\n",
            "    (an integer)\n",
            "  --epsilon: epsilon parameter of Adam optimizer\n",
            "    (default: '1e-08')\n",
            "    (a number)\n",
            "  --es_epochs: Number of epochs with no improvement after which training will be\n",
            "    stopped. Loss is not stored in the checkpoint so when checkpoint is revived\n",
            "    it starts the loss calculation from start at that point\n",
            "    (default: '25')\n",
            "    (an integer)\n",
            "  --es_min_delta: Minimum change in loss to qualify as an improvement. This\n",
            "    value will also be used in Reduce learning rate on plateau\n",
            "    (default: '0.05')\n",
            "    (a number)\n",
            "  --export_author_id: author of the exported model. GitHub user or organization\n",
            "    name used to uniquely identify the author of this model\n",
            "    (default: 'author')\n",
            "  --export_batch_size: number of elements per batch on the exported graph\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --export_beam_width: default beam width to embed into exported graph\n",
            "    (default: '500')\n",
            "    (an integer)\n",
            "  --export_contact_info: public contact information of the author. Can be an\n",
            "    email address, or a link to a contact form, issue tracker, or discussion\n",
            "    forum. Must provide a way to reach the model authors\n",
            "    (default: '<public contact information of the author. Can be an email\n",
            "    address, or a link to a contact form, issue tracker, or discussion forum.\n",
            "    Must provide a way to reach the model authors>')\n",
            "  --export_description: Freeform description of the model being exported.\n",
            "    Markdown accepted. You can also leave this flag unchanged and edit the\n",
            "    generated .md file directly. Useful things to describe are demographic and\n",
            "    acoustic characteristics of the data used to train the model, any\n",
            "    architectural changes, names of public datasets that were used when\n",
            "    applicable, hyperparameters used for training, evaluation results on\n",
            "    standard benchmark datasets, etc.\n",
            "    (default: '<Freeform description of the model being exported. Markdown\n",
            "    accepted. You can also leave this flag unchanged and edit the generated .md\n",
            "    file directly. Useful things to describe are demographic and acoustic\n",
            "    characteristics of the data used to train the model, any architectural\n",
            "    changes, names of public datasets that were used when applicable,\n",
            "    hyperparameters used for training, evaluation results on standard benchmark\n",
            "    datasets, etc.>')\n",
            "  --export_dir: directory in which exported models are stored - if omitted, the\n",
            "    model won't get exported\n",
            "    (default: '')\n",
            "  --export_file_name: name for the exported model file name\n",
            "    (default: 'output_graph')\n",
            "  --export_language: language the model was trained on - IETF BCP 47 language\n",
            "    tag including at least language, script and region subtags. E.g. \"en-Latn-\n",
            "    UK\" or \"de-Latn-DE\" or \"cmn-Hans-CN\". Include as much info as you can\n",
            "    without loss of precision. For example, if a model is trained on Scottish\n",
            "    English, include the variant subtag: \"en-Latn-GB-Scotland\".\n",
            "    (default: '<language the model was trained on - IETF BCP 47 language tag\n",
            "    including at least language, script and region subtags. E.g. \"en-Latn-UK\" or\n",
            "    \"de-Latn-DE\" or \"cmn-Hans-CN\". Include as much info as you can without loss\n",
            "    of precision. For example, if a model is trained on Scottish English,\n",
            "    include the variant subtag: \"en-Latn-GB-Scotland\".>')\n",
            "  --export_license: SPDX identifier of the license of the exported model. See\n",
            "    https://spdx.org/licenses/. If the license does not have an SPDX identifier,\n",
            "    use the license name.\n",
            "    (default: '<SPDX identifier of the license of the exported model. See\n",
            "    https://spdx.org/licenses/. If the license does not have an SPDX identifier,\n",
            "    use the license name.>')\n",
            "  --export_max_ds_version: maximum DeepSpeech version (inclusive) the exported\n",
            "    model is compatible with\n",
            "    (default: '<maximum DeepSpeech version (inclusive) the exported model is\n",
            "    compatible with>')\n",
            "  --export_min_ds_version: minimum DeepSpeech version (inclusive) the exported\n",
            "    model is compatible with\n",
            "    (default: '<minimum DeepSpeech version (inclusive) the exported model is\n",
            "    compatible with>')\n",
            "  --export_model_name: name of the exported model. Must not contain forward\n",
            "    slashes.\n",
            "    (default: 'model')\n",
            "  --export_model_version: semantic version of the exported model. See\n",
            "    https://semver.org/. This is fully controlled by you as author of the model\n",
            "    and has no required connection with DeepSpeech versions\n",
            "    (default: '0.0.1')\n",
            "  --[no]export_tflite: export a graph ready for TF Lite engine\n",
            "    (default: 'false')\n",
            "  --[no]export_zip: export a TFLite model and package with LM and info.json\n",
            "    (default: 'false')\n",
            "  --feature_cache: cache MFCC features to disk to speed up future training runs\n",
            "    on the same data. This flag specifies the path where cached features\n",
            "    extracted from --train_files will be saved. If empty, or if online\n",
            "    augmentation flags are enabled, caching will be disabled.\n",
            "    (default: '')\n",
            "  --feature_win_len: feature extraction audio window length in milliseconds\n",
            "    (default: '32')\n",
            "    (an integer)\n",
            "  --feature_win_step: feature extraction window step length in milliseconds\n",
            "    (default: '20')\n",
            "    (an integer)\n",
            "  --[no]force_initialize_learning_rate: Force re-initialization of learning rate\n",
            "    which was previously reduced.\n",
            "    (default: 'false')\n",
            "  --inter_op_parallelism_threads: number of inter-op parallelism threads - see\n",
            "    tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --intra_op_parallelism_threads: number of intra-op parallelism threads - see\n",
            "    tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --[no]layer_norm: wether to use layer-normalization after each fully-connected\n",
            "    layer (except the last one)\n",
            "    (default: 'false')\n",
            "  --learning_rate: learning rate of Adam optimizer\n",
            "    (default: '0.001')\n",
            "    (a number)\n",
            "  --limit_dev: maximum number of elements to use from validation set - 0 means\n",
            "    no limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --limit_test: maximum number of elements to use from test set - 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --limit_train: maximum number of elements to use from train set - 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --lm_alpha: the alpha hyperparameter of the CTC decoder. Language Model\n",
            "    weight.\n",
            "    (default: '0.931289039105002')\n",
            "    (a number)\n",
            "  --lm_alpha_max: the maximum of the alpha hyperparameter of the CTC decoder\n",
            "    explored during hyperparameter optimization. Language Model weight.\n",
            "    (default: '5.0')\n",
            "    (a number)\n",
            "  --lm_beta: the beta hyperparameter of the CTC decoder. Word insertion weight.\n",
            "    (default: '1.1834137581510284')\n",
            "    (a number)\n",
            "  --lm_beta_max: the maximum beta hyperparameter of the CTC decoder explored\n",
            "    during hyperparameter optimization. Word insertion weight.\n",
            "    (default: '5.0')\n",
            "    (a number)\n",
            "  --load_checkpoint_dir: directory in which checkpoints are stored - defaults to\n",
            "    directory \"deepspeech/checkpoints\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --[no]load_cudnn: Specifying this flag allows one to convert a CuDNN RNN\n",
            "    checkpoint to a checkpoint capable of running on a CPU graph.\n",
            "    (default: 'false')\n",
            "  --load_evaluate: what checkpoint to load for evaluation tasks (test epochs,\n",
            "    model export, single file inference, etc). \"last\" for loading most recent\n",
            "    epoch checkpoint, \"best\" for loading best validation loss checkpoint, \"auto\"\n",
            "    for trying several options.\n",
            "    (default: 'auto')\n",
            "  --load_train: what checkpoint to load before starting the training process.\n",
            "    \"last\" for loading most recent epoch checkpoint, \"best\" for loading best\n",
            "    validation loss checkpoint, \"init\" for initializing a new checkpoint, \"auto\"\n",
            "    for trying several options.\n",
            "    (default: 'auto')\n",
            "  --log_level: log level for console logs - 0: DEBUG, 1: INFO, 2: WARN, 3: ERROR\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --[no]log_placement: whether to log device placement of the operators to the\n",
            "    console\n",
            "    (default: 'false')\n",
            "  --max_to_keep: number of checkpoint files to keep - default value is 5\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --metrics_files: comma separated list of files specifying the datasets used\n",
            "    for tracking of metrics (after validation step). Currently the only metric\n",
            "    is the CTC loss but without affecting the tracking of best validation loss.\n",
            "    Multiple files will get reported separately. If empty, metrics will not be\n",
            "    computed.\n",
            "    (default: '')\n",
            "  --n_hidden: layer width to use when initialising layers\n",
            "    (default: '2048')\n",
            "    (an integer)\n",
            "  --n_steps: how many timesteps to process at once by the export graph, higher\n",
            "    values mean more latency\n",
            "    (default: '16')\n",
            "    (an integer)\n",
            "  --n_trials: the number of trials to run during hyperparameter optimization.\n",
            "    (default: '2400')\n",
            "    (an integer)\n",
            "  --one_shot_infer: one-shot inference mode: specify a wav file and the script\n",
            "    will load the checkpoint and perform inference on it.\n",
            "    (default: '')\n",
            "  --plateau_epochs: Number of epochs to consider for RLROP. Has to be smaller\n",
            "    than es_epochs from early stopping\n",
            "    (default: '10')\n",
            "    (an integer)\n",
            "  --plateau_reduction: Multiplicative factor to apply to the current learning\n",
            "    rate if a plateau has occurred.\n",
            "    (default: '0.1')\n",
            "    (a number)\n",
            "  --random_seed: default random seed that is used to initialize variables\n",
            "    (default: '4568')\n",
            "    (an integer)\n",
            "  --read_buffer: buffer-size for reading samples from datasets (supports file-\n",
            "    size suffixes KB, MB, GB, TB)\n",
            "    (default: '1MB')\n",
            "  --[no]reduce_lr_on_plateau: Enable reducing the learning rate if a plateau is\n",
            "    reached. This is the case if the validation loss did not improve for some\n",
            "    epochs.\n",
            "    (default: 'false')\n",
            "  --relu_clip: ReLU clipping value for non-recurrent layers\n",
            "    (default: '20.0')\n",
            "    (a number)\n",
            "  --[no]remove_export: whether to remove old exported models\n",
            "    (default: 'false')\n",
            "  --report_count: number of phrases for each of best WER, median WER and worst\n",
            "    WER to print out during a WER report\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --[no]reverse_dev: if to reverse sample order of the dev set\n",
            "    (default: 'false')\n",
            "  --[no]reverse_test: if to reverse sample order of the test set\n",
            "    (default: 'false')\n",
            "  --[no]reverse_train: if to reverse sample order of the train set\n",
            "    (default: 'false')\n",
            "  --save_checkpoint_dir: directory to which checkpoints are saved - defaults to\n",
            "    directory \"deepspeech/checkpoints\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --scorer: Alias for --scorer_path.\n",
            "    (default: '')\n",
            "  --scorer_path: path to the external scorer file.\n",
            "    (default: '')\n",
            "  --[no]show_progressbar: Show progress for training, validation and testing\n",
            "    processes. Log level should be > 0.\n",
            "    (default: 'true')\n",
            "  --summary_dir: target directory for TensorBoard summaries - defaults to\n",
            "    directory \"deepspeech/summaries\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --test_batch_size: number of elements in a test batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --test_files: comma separated list of files specifying the datasets used for\n",
            "    testing. Multiple files will get reported separately. If empty, the model\n",
            "    will not be tested.\n",
            "    (default: '')\n",
            "  --test_output_file: path to a file to save all src/decoded/distance/loss\n",
            "    tuples generated during a test epoch\n",
            "    (default: '')\n",
            "  --train_batch_size: number of elements in a training batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --[no]train_cudnn: use CuDNN RNN backend for training on GPU. Note that\n",
            "    checkpoints created with this flag can only be used with CuDNN RNN, i.e.\n",
            "    fine tuning on a CPU device will not work\n",
            "    (default: 'false')\n",
            "  --train_files: comma separated list of files specifying the dataset used for\n",
            "    training. Multiple files will get merged. If empty, training will not be\n",
            "    run.\n",
            "    (default: '')\n",
            "  --[no]use_allow_growth: use Allow Growth flag which will allocate only\n",
            "    required amount of GPU memory and prevent full allocation of available GPU\n",
            "    memory\n",
            "    (default: 'false')\n",
            "\n",
            "tensorflow.python.ops.parallel_for.pfor:\n",
            "  --[no]op_conversion_fallback_to_while_loop: If true, falls back to using a\n",
            "    while loop for ops for which a converter is not defined.\n",
            "    (default: 'false')\n",
            "\n",
            "absl.flags:\n",
            "  --flagfile: Insert flag definitions from the given file into the command line.\n",
            "    (default: '')\n",
            "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
            "    the command line even if the program does not define a flag with that name.\n",
            "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
            "    format.\n",
            "    (default: '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### STEP 2: DOWNLOAD AND PREPARE THE DATASET FOR TRAINING"
      ],
      "metadata": {
        "id": "xC-EG26wiSxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "%cd '/content'\n",
        "!git clone https://github.com/csikasote/BembaSpeech.git "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp94ivqkiR8A",
        "outputId": "72c68ec3-7b0e-4155-a512-e99ecd03bf2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'BembaSpeech'...\n",
            "remote: Enumerating objects: 35754, done.\u001b[K\n",
            "remote: Counting objects: 100% (371/371), done.\u001b[K\n",
            "remote: Compressing objects: 100% (239/239), done.\u001b[K\n",
            "remote: Total 35754 (delta 170), reused 304 (delta 130), pack-reused 35383\n",
            "Receiving objects: 100% (35754/35754), 2.44 GiB | 17.50 MiB/s, done.\n",
            "Resolving deltas: 100% (17742/17742), done.\n",
            "Checking out files: 100% (14444/14444), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abs_path_to_data = \"/content/BembaSpeech/data\"\n",
        "!ls {abs_path_to_data}/splits/*.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4mFpjO0ihv5",
        "outputId": "05174784-3ba4-4319-96a6-8b6957746f4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BembaSpeech/data/splits/dev.csv\n",
            "/content/BembaSpeech/data/splits/test.csv\n",
            "/content/BembaSpeech/data/splits/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/BembaSpeech/data/splits/train.csv', sep=\",\")\n",
        "train_df.to_csv(f\"{abs_path_to_data}/splits/train.csv\", sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "zeC7lpFwihzB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(abs_path_to_data):\n",
        "  splits=glob(f\"{abs_path_to_data}/splits/*.csv\")\n",
        "  split_1 = os.path.basename(splits[0])[:-4]\n",
        "  split_2 = os.path.basename(splits[1])[:-4]\n",
        "  split_3 = os.path.basename(splits[2])[:-4]\n",
        "\n",
        "  split_1_df = pd.read_csv(splits[0], sep=\"\\t\")\n",
        "  split_1_df[\"path\"] = abs_path_to_data + \"/audio/\" + split_1_df['audio']\n",
        "  split_1_df[\"status\"] = split_1_df[\"path\"].apply(lambda path: True if os.path.exists(path) else None)\n",
        "  split_1_df[\"wav_filesize\"] = split_1_df[\"path\"].apply(lambda path: os.path.getsize(path) if os.path.exists(path) else None)\n",
        "  split_1_df = split_1_df.dropna(subset=[\"path\"])\n",
        "  split_1_df[\"transcript\"] = split_1_df['sentence']\n",
        "  split_1_df = split_1_df[(split_1_df.duration<=10) & (split_1_df.duration>1)]\n",
        "  split_1_df = split_1_df.drop(columns=['audio','status','duration', 'sentence'])\n",
        "  split_1_df = split_1_df.rename(columns={'path':'wav_filename'})\n",
        "  split_1_df.to_csv(f\"{abs_path_to_data}/splits/{split_1}.csv\", sep=',', index=False)\n",
        "  print(f\"No. of {split_1} records: {len(split_1_df)}\")\n",
        "\n",
        "  split_2_df = pd.read_csv(splits[1], sep=\"\\t\")\n",
        "  split_2_df[\"path\"] = abs_path_to_data + \"/audio/\" + split_2_df['audio']\n",
        "  split_2_df[\"status\"] = split_2_df[\"path\"].apply(lambda path: True if os.path.exists(path) else None)\n",
        "  split_2_df[\"wav_filesize\"] = split_2_df[\"path\"].apply(lambda path: os.path.getsize(path) if os.path.exists(path) else None)\n",
        "  split_2_df = split_2_df.dropna(subset=[\"path\"])\n",
        "  split_2_df[\"transcript\"] = split_2_df['sentence']\n",
        "  split_2_df = split_2_df[(split_2_df.duration<=10) & (split_2_df.duration>1)]\n",
        "  split_2_df = split_2_df.drop(columns=['audio','status','duration', 'sentence'])\n",
        "  split_2_df = split_2_df.rename(columns={'path':'wav_filename'})\n",
        "  split_2_df.to_csv(f\"{abs_path_to_data}/splits/{split_2}.csv\", sep=',', index=False)\n",
        "  print(f\"No. of {split_2} records: {len(split_2_df)}\")\n",
        "\n",
        "  split_3_df = pd.read_csv(splits[2], sep=\"\\t\")\n",
        "  split_3_df[\"path\"] = abs_path_to_data + \"/audio/\" + split_3_df['audio']\n",
        "  split_3_df[\"status\"] = split_3_df[\"path\"].apply(lambda path: True if os.path.exists(path) else None)\n",
        "  split_3_df[\"wav_filesize\"] = split_3_df[\"path\"].apply(lambda path: os.path.getsize(path) if os.path.exists(path) else None)\n",
        "  split_3_df = split_3_df.dropna(subset=[\"path\"])\n",
        "  split_3_df[\"transcript\"] = split_3_df['sentence']\n",
        "  split_3_df = split_3_df[(split_3_df.duration<=10) & (split_3_df.duration>1)]\n",
        "  split_3_df = split_3_df.drop(columns=['audio','status','duration', 'sentence'])\n",
        "  split_3_df = split_3_df.rename(columns={'path':'wav_filename'})\n",
        "  split_3_df.to_csv(f\"{abs_path_to_data}/splits/{split_3}.csv\", sep=',', index=False)\n",
        "  print(f\"No. of {split_3} records: {len(split_3_df)}\")"
      ],
      "metadata": {
        "id": "fmDmjffMih1x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_dataset(abs_path_to_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAGg4UP7izWT",
        "outputId": "c99d5788-cda6-4de7-d9c6-6cb099ff40f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of dev records: 1437\n",
            "No. of test records: 756\n",
            "No. of train records: 10192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBqlRqyqRwr-"
      },
      "source": [
        "PHASE 3: MODEL FINETUNING THE DEEPSPEECH MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSpsNJQOkbrs",
        "outputId": "e8523fe7-5dc6-4fe3-e9a4-6e6516dff7ef"
      },
      "source": [
        "# STEP 1: DOWNLOAD AND EXTRACT DEEPSPEECHV0.9.3 CHECKPOINTS\n",
        "%cd '/content' # you canchange the directory to which you want to make a download to\n",
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-checkpoint.tar.gz\n",
        "!tar xvfz deepspeech-0.9.3-checkpoint.tar.gz\n",
        "!rm -rf xvfz deepspeech-0.9.3-checkpoint.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2022-09-08 14:35:15--  https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-checkpoint.tar.gz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/60273704/6598e800-3b0f-11eb-9e91-3db57dd0c70b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220908%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220908T143515Z&X-Amz-Expires=300&X-Amz-Signature=a169a878d8e0f0e197c7fd1609ef4dfdf96fd5bd377f043f7deb47b02d8f5756&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.9.3-checkpoint.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-09-08 14:35:15--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/60273704/6598e800-3b0f-11eb-9e91-3db57dd0c70b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220908%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220908T143515Z&X-Amz-Expires=300&X-Amz-Signature=a169a878d8e0f0e197c7fd1609ef4dfdf96fd5bd377f043f7deb47b02d8f5756&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.9.3-checkpoint.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 645992216 (616M) [application/octet-stream]\n",
            "Saving to: ‘deepspeech-0.9.3-checkpoint.tar.gz’\n",
            "\n",
            "deepspeech-0.9.3-ch  25%[====>               ] 158.72M  --.-KB/s    in 26s     \n",
            "\n",
            "2022-09-08 14:35:43 (6.08 MB/s) - Connection closed at byte 166428672. Retrying.\n",
            "\n",
            "--2022-09-08 14:35:44--  (try: 2)  https://objects.githubusercontent.com/github-production-release-asset-2e65be/60273704/6598e800-3b0f-11eb-9e91-3db57dd0c70b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220908%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220908T143515Z&X-Amz-Expires=300&X-Amz-Signature=a169a878d8e0f0e197c7fd1609ef4dfdf96fd5bd377f043f7deb47b02d8f5756&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.9.3-checkpoint.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 645992216 (616M), 479563544 (457M) remaining [application/octet-stream]\n",
            "Saving to: ‘deepspeech-0.9.3-checkpoint.tar.gz’\n",
            "\n",
            "deepspeech-0.9.3-ch 100%[+++++==============>] 616.07M  12.1MB/s    in 65s     \n",
            "\n",
            "2022-09-08 14:36:50 (6.99 MB/s) - ‘deepspeech-0.9.3-checkpoint.tar.gz’ saved [645992216/645992216]\n",
            "\n",
            "deepspeech-0.9.3-checkpoint/\n",
            "deepspeech-0.9.3-checkpoint/flags.txt\n",
            "deepspeech-0.9.3-checkpoint/best_dev-1466475.meta\n",
            "deepspeech-0.9.3-checkpoint/best_dev-1466475.index\n",
            "deepspeech-0.9.3-checkpoint/best_dev_checkpoint\n",
            "deepspeech-0.9.3-checkpoint/best_dev-1466475.data-00000-of-00001\n",
            "deepspeech-0.9.3-checkpoint/checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be sure to create the `export` and `summary` in the `content` directories."
      ],
      "metadata": {
        "id": "bwULpYDuogxE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRcEaw9VRSh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8285544b-e1cc-48bb-aa88-368b3fa79608"
      },
      "source": [
        "# STEP 2: FINETUNE ACOUSTIC MODEL WITH DEEPSPEECHV0.9.3 (-LM)\n",
        "!python3 /content/DeepSpeech/DeepSpeech.py \\\n",
        "  --train_files /content/BembaSpeech/data/splits/train.csv \\\n",
        "  --dev_files /content/BembaSpeech/data/splits/dev.csv \\\n",
        "  --test_files /content/BembaSpeech/data/splits/test.csv \\\n",
        "  --checkpoint_dir /content/deepspeech-0.9.3-checkpoint \\\n",
        "  --alphabet_config_path /content/DeepSpeech/data/alphabet.txt \\\n",
        "  --export_dir /content/export \\\n",
        "  --summary_dir /content/summary \\\n",
        "  --learning_rate 0.0005 \\\n",
        "  --dropout_rate 0.4 \\\n",
        "  --train_batch_size 64 \\\n",
        "  --test_batch_size 32 \\\n",
        "  --dev_batch_size 32 \\\n",
        "  --train_cudnn True \\\n",
        "  --early_stop True \\\n",
        "  --n_hidden 2048 \\\n",
        "  --es_epochs 2 \\\n",
        "  --epochs 2 \\\n",
        "  --export_file_name 'ft_model' \\\n",
        "  --export_author_id 'csikasote' \\\n",
        "  --augment reverb[p=0.2,delay=50.0~30.0,decay=10.0:2.0~1.0] \\\n",
        "  --augment volume[p=0.2,dbfs=-10:-40] \\\n",
        "  --augment pitch[p=0.2,pitch=1~0.2] \\\n",
        "  --augment tempo[p=0.2,factor=1~0.5] "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Loading best validating checkpoint from /content/deepspeech-0.9.3-checkpoint/best_dev-1466475\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/opaque_kernel\n",
            "I Loading variable from checkpoint: cudnn_lstm/opaque_kernel/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/opaque_kernel/Adam_1\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:05:21 | Steps: 159 | Loss: 119.959118    \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:16 | Steps: 45 | Loss: 71.892508 | Dataset: /content/BembaSpeech/data/splits/dev.csv\n",
            "I Saved new best validating model with loss 71.892508 to: /content/deepspeech-0.9.3-checkpoint/best_dev-1466634\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:05:16 | Steps: 159 | Loss: 95.250195     \n",
            "Epoch 1 | Validation | Elapsed Time: 0:00:15 | Steps: 45 | Loss: 59.619557 | Dataset: /content/BembaSpeech/data/splits/dev.csv\n",
            "I Saved new best validating model with loss 59.619557 to: /content/deepspeech-0.9.3-checkpoint/best_dev-1466793\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:11:21.822429\n",
            "I Loading best validating checkpoint from /content/deepspeech-0.9.3-checkpoint/best_dev-1466793\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/BembaSpeech/data/splits/test.csv\n",
            "Test epoch | Steps: 24 | Elapsed Time: 0:19:55                                  \n",
            "Test on /content/BembaSpeech/data/splits/test.csv - WER: 0.982633, CER: 0.392459, loss: 68.536331\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.500000, CER: 0.250000, loss: 14.481583\n",
            " - wav: file:///content/BembaSpeech/data/audio/06-200917-110207_bem_c20_elicit_21.wav\n",
            " - src: \"abati kabiye\"\n",
            " - res: \"abati fabi\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.500000, CER: 0.250000, loss: 9.776178\n",
            " - wav: file:///content/BembaSpeech/data/audio/06-200918-153816_bem_c20_elicit_35.wav\n",
            " - src: \"afika atoota\"\n",
            " - res: \"afika atobak\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.500000, CER: 0.222222, loss: 7.943928\n",
            " - wav: file:///content/BembaSpeech/data/audio/06-200916-152623_bem_c20_elicit_25.wav\n",
            " - src: \"awe afika\"\n",
            " - res: \"awe asikla\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.500000, CER: 0.090909, loss: 6.341507\n",
            " - wav: file:///content/BembaSpeech/data/audio/06-200916-094743_bem_c20_elicit_47.wav\n",
            " - src: \"abati iseni\"\n",
            " - res: \"abati isani\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.500000, CER: 0.066667, loss: 6.147079\n",
            " - wav: file:///content/BembaSpeech/data/audio/12-201008-130449_bem_526_elicit_0.wav\n",
            " - src: \"inkalamo shaisa\"\n",
            " - res: \"inkalamu shaisa\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.376812, loss: 74.437828\n",
            " - wav: file:///content/BembaSpeech/data/audio/06-200916-150125_bem_c20_elicit_47.wav\n",
            " - src: \"mukwai kaluka ulya wine maayo kalukile pali mumbi uwawama nga malaika\"\n",
            " - res: \"nawaiganukalainemayo kakila palimuli waba magamalaka\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.320000, loss: 74.311897\n",
            " - wav: file:///content/BembaSpeech/data/audio/12-201010-081738_bem_526_elicit_19.wav\n",
            " - src: \"nomba nalatontonkanya ico banjitila na ine ku milandu ne ushalelwako lubuli\"\n",
            " - res: \"noba nalatontonkanetawantilanaini kuilandu neushaliwakonuwud\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.291667, loss: 73.949860\n",
            " - wav: file:///content/BembaSpeech/data/audio/12-201007-175803_bem_526_elicit_10.wav\n",
            " - src: \"ulucelo ico abapatili bali no kulanda na bantu  bonse balongana ku isano\"\n",
            " - res: \"olocelo ica rabatilubalinoklandanabantu bolsabalo nbana krisano\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.337662, loss: 73.771271\n",
            " - wav: file:///content/BembaSpeech/data/audio/06-200917-130741_bem_c20_elicit_29.wav\n",
            " - src: \"aikasha na ukuboko kwamwana aputula afumishako na umulimi ikoosa lyakwa mwane\"\n",
            " - res: \"i kashana okubakokwamana atula arkinishaakunaumulini ukosaleakwabuan\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.358974, loss: 73.748543\n",
            " - wav: file:///content/BembaSpeech/data/audio/06-200916-110927_bem_c20_elicit_1.wav\n",
            " - src: \"kalaikanga na kumulilo akati imfula ya uno mwaka nayo ileloka na amalumba tata\"\n",
            " - res: \"calakanganapomuliloakafi ukuleano makamaailelokana malonbataa\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.300000, loss: 7.655228\n",
            " - wav: file:///content/BembaSpeech/data/audio/06-200917-130741_bem_c20_elicit_45.wav\n",
            " - src: \"akantu ako\"\n",
            " - res: \"akaantoako\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.250000, CER: 0.304348, loss: 50.944942\n",
            " - wav: file:///content/BembaSpeech/data/audio/12-201007-230806_bem_526_elicit_17.wav\n",
            " - src: \"abapatili babwelelamo  amacushi yalanangulusha\"\n",
            " - res: \"aabatili bawililabu hamachuushi ialana molusha\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.500000, CER: 0.533333, loss: 28.792742\n",
            " - wav: file:///content/BembaSpeech/data/audio/12-201010-100842_bem_526_elicit_0.wav\n",
            " - src: \"ukuyamone mfumu\"\n",
            " - res: \"i ukuia munimfume\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.500000, CER: 0.187500, loss: 14.003556\n",
            " - wav: file:///content/BembaSpeech/data/audio/12-201007-230806_bem_526_elicit_18.wav\n",
            " - src: \"twaisula isukulu\"\n",
            " - res: \"t asula issukulu\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.500000, CER: 0.250000, loss: 12.848349\n",
            " - wav: file:///content/BembaSpeech/data/audio/12-201009-113239_bem_526_elicit_0.wav\n",
            " - src: \"twaisula isukulu\"\n",
            " - res: \"t isula isukolun\"\n",
            "--------------------------------------------------------------------------------\n",
            "I Exporting the model...\n",
            "I Loading best validating checkpoint from /content/deepspeech-0.9.3-checkpoint/best_dev-1466793\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Models exported at /content/export\n",
            "I Model metadata file saved to /content/export/csikasote_model_0.0.1.md. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb2ISHPQaHah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf04704e-0d73-482b-caf6-92795f181efb"
      },
      "source": [
        "# STEP 3: CREATE A MMAPABLE MODEL\n",
        "%cd '/content/DeepSpeech/'\n",
        "!python3 util/taskcluster.py \\\n",
        "  --source tensorflow \\\n",
        "  --artifact convert_graphdef_memmapped_format \\\n",
        "  --branch r1.15 \\\n",
        "  --target ."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepSpeech\n",
            "Downloading https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.tensorflow.pip.r1.15.cpu/artifacts/public/convert_graphdef_memmapped_format ...\n",
            "Downloading: 100%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkhV-TiBN094",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f08cd4d-a0e9-40e2-84eb-5856cbebb8a5"
      },
      "source": [
        "%cd '/content/DeepSpeech/'\n",
        "!./convert_graphdef_memmapped_format \\\n",
        "  --in_graph='/content/export/ft_model.pb' \\\n",
        "  --out_graph='/content/ft_model.pbmm'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepSpeech\n",
            "2022-09-08 15:27:20.355295: I tensorflow/contrib/util/convert_graphdef_memmapped_format_lib.cc:171] Converted 7 nodes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDZtyPc-mqSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}